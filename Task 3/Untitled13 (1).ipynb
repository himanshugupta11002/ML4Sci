{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "dZPQU-2ntJDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Task 1***"
      ],
      "metadata": {
        "id": "fZ2pa-g8Atmv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sympy as sp\n",
        "\n",
        "\n",
        "x = sp.symbols('x')\n",
        "\n",
        "functions = [\n",
        "    x**2,                           # Polynomial function\n",
        "    sp.sin(x),                      # Trigonometric functions\n",
        "    sp.cos(x),\n",
        "    sp.tan(x),\n",
        "    sp.exp(x),                      # Exponential and logarithmic functions\n",
        "    sp.log(1 + x),\n",
        "    sp.log(x + 2),\n",
        "    sp.cosh(x),                     # Hyperbolic functions\n",
        "    sp.sinh(x),\n",
        "    sp.tanh(x),\n",
        "    sp.erf(x),                      # Special functions\n",
        "    sp.erfc(x),\n",
        "    sp.gamma(x),\n",
        "    sp.beta(x + 1, 2),\n",
        "    sp.sin(x**2),                   # Composite functions\n",
        "    sp.exp(x**2),\n",
        "    sp.log(1 + x**2),\n",
        "    sp.diff(sp.exp(x), x),          # Differentiation\n",
        "    sp.integrate(sp.exp(x), x),     # Integration\n",
        "    sp.sqrt(x),                     # Square root\n",
        "    sp.atan(x),                     # Arctan\n",
        "    sp.acos(x),                     # Arccos\n",
        "    sp.acosh(x),                    # Hyperbolic arccos\n",
        "    sp.DiracDelta(x),               # Dirac delta function\n",
        "    sp.Heaviside(x),                # Heaviside step function\n",
        "    sp.factorial(x),                # Factorial function\n",
        "    sp.besselj(1, x),               # Bessel function of the first kind\n",
        "    sp.jacobi(1, 1, x,1),             # Jacobi function\n",
        "    sp.legendre(2, x),              # Legendre polynomial\n",
        "    sp.hankel1(1, x),               # Hankel function of the first kind\n",
        "    sp.Ynm(2, 1, x, 1),                # Spherical Bessel function of the second kind\n",
        "    sp.fresnels(x),                 # Fresnel sine integral\n",
        "    sp.fresnelc(x),                 # Fresnel cosine integral\n",
        "    sp.airyai(x),                   # Airy function of the first kind\n",
        "    sp.airybi(x),                   # Airy function of the second kind\n",
        "\n",
        "    sp.zeta(x),                     # Riemann zeta function\n",
        "    sp.dirichlet_eta(x)             # Dirichlet eta function\n",
        "]\n",
        "\n",
        "\n",
        "max_order = 4\n",
        "\n",
        "\n",
        "taylor_expansions = {}\n",
        "for func in functions:\n",
        "    try:\n",
        "        taylor_expansions[str(func)] = [func.series(x, 0, n).removeO() for n in range(1, max_order + 1)]\n",
        "    except sp.PoleError:\n",
        "        continue\n",
        "\n",
        "\n",
        "taylor_expansions = {key: [str(expansion) for expansion in value] for key, value in taylor_expansions.items()}\n",
        "\n",
        "\n",
        "with open('taylor_dataset.txt', 'w') as file:\n",
        "    for func, expansions in taylor_expansions.items():\n",
        "        file.write(f'Function: {func}\\n')\n",
        "        for order, expansion in enumerate(expansions, start=1):\n",
        "            file.write(f'Order {order} Taylor Expansion: {expansion}\\n')\n",
        "        file.write('\\n')\n",
        "\n",
        "print(\"Dataset saved to 'taylor_dataset.txt'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cq2fci1ds00U",
        "outputId": "cdc19777-08da-4c38-dcc3-845b0b602204"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset saved to 'taylor_dataset.txt'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***TASK 2***"
      ],
      "metadata": {
        "id": "L7q-Mh5uAfFc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding, Bidirectional, Dropout\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Load the dataset\n",
        "dataset_file = 'taylor_dataset.txt'\n",
        "with open(dataset_file, 'r') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "# Prepare data\n",
        "max_words = 10000  # Maximum number of words to consider as features\n",
        "max_len = 100  # Maximum length of each sequence\n",
        "\n",
        "# Tokenize the text\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(lines)\n",
        "\n",
        "# Convert text to sequences\n",
        "sequences = tokenizer.texts_to_sequences(lines)\n",
        "\n",
        "# Pad sequences to ensure uniform length\n",
        "data = pad_sequences(sequences, maxlen=max_len)\n",
        "\n",
        "# Split data into input and target\n",
        "x_data = data[:, :-1]\n",
        "y_data = data[:, -1]\n",
        "\n",
        "# Build the LSTM model\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, 100, input_length=max_len - 1))\n",
        "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Bidirectional(LSTM(128)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(max_words, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_data, y_data, epochs=100, verbose=1, validation_split=0.2)\n",
        "\n",
        "# Save the trained model\n",
        "model.save('taylor_expansion_lstm_model.h5')\n",
        "\n",
        "# Print final accuracy and loss\n",
        "loss, accuracy = model.evaluate(x_data, y_data, verbose=0)\n",
        "print(f'Final Loss: {loss:.4f}')\n",
        "print(f'Final Accuracy: {accuracy:.4f}')\n",
        "\n",
        "print(\"LSTM model trained and saved successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whYcej6051QU",
        "outputId": "99fe100d-19b5-4835-c164-db3da34dea9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "6/6 [==============================] - 20s 2s/step - loss: 9.0443 - accuracy: 0.2381 - val_loss: 8.0120 - val_accuracy: 0.1905\n",
            "Epoch 2/100\n",
            "6/6 [==============================] - 10s 2s/step - loss: 6.8765 - accuracy: 0.2679 - val_loss: 5.2209 - val_accuracy: 0.1905\n",
            "Epoch 3/100\n",
            "6/6 [==============================] - 8s 1s/step - loss: 3.6874 - accuracy: 0.2619 - val_loss: 2.8166 - val_accuracy: 0.1905\n",
            "Epoch 4/100\n",
            "6/6 [==============================] - 10s 2s/step - loss: 1.9446 - accuracy: 0.2560 - val_loss: 2.4667 - val_accuracy: 0.2143\n",
            "Epoch 5/100\n",
            "6/6 [==============================] - 10s 2s/step - loss: 1.7041 - accuracy: 0.2857 - val_loss: 2.4734 - val_accuracy: 0.2143\n",
            "Epoch 6/100\n",
            "6/6 [==============================] - 9s 2s/step - loss: 1.6960 - accuracy: 0.2500 - val_loss: 2.4919 - val_accuracy: 0.2143\n",
            "Epoch 7/100\n",
            "6/6 [==============================] - 9s 1s/step - loss: 1.6831 - accuracy: 0.2857 - val_loss: 2.4329 - val_accuracy: 0.2143\n",
            "Epoch 8/100\n",
            "6/6 [==============================] - 11s 2s/step - loss: 1.6845 - accuracy: 0.2440 - val_loss: 2.3879 - val_accuracy: 0.2143\n",
            "Epoch 9/100\n",
            "6/6 [==============================] - 10s 2s/step - loss: 1.6828 - accuracy: 0.2679 - val_loss: 2.3636 - val_accuracy: 0.2143\n",
            "Epoch 10/100\n",
            "6/6 [==============================] - 9s 2s/step - loss: 1.6770 - accuracy: 0.2976 - val_loss: 2.4057 - val_accuracy: 0.1905\n",
            "Epoch 11/100\n",
            "6/6 [==============================] - 9s 1s/step - loss: 1.6705 - accuracy: 0.2679 - val_loss: 2.4111 - val_accuracy: 0.2143\n",
            "Epoch 12/100\n",
            "6/6 [==============================] - 10s 2s/step - loss: 1.6807 - accuracy: 0.2619 - val_loss: 2.3632 - val_accuracy: 0.2143\n",
            "Epoch 13/100\n",
            "6/6 [==============================] - 10s 2s/step - loss: 1.6755 - accuracy: 0.2440 - val_loss: 2.3727 - val_accuracy: 0.1905\n",
            "Epoch 14/100\n",
            "6/6 [==============================] - 9s 1s/step - loss: 1.6813 - accuracy: 0.2679 - val_loss: 2.3963 - val_accuracy: 0.2143\n",
            "Epoch 15/100\n",
            "6/6 [==============================] - 10s 1s/step - loss: 1.6774 - accuracy: 0.2679 - val_loss: 2.4175 - val_accuracy: 0.1905\n",
            "Epoch 16/100\n",
            "6/6 [==============================] - 10s 2s/step - loss: 1.6822 - accuracy: 0.2560 - val_loss: 2.4312 - val_accuracy: 0.1905\n",
            "Epoch 17/100\n",
            "6/6 [==============================] - 10s 2s/step - loss: 1.6814 - accuracy: 0.2321 - val_loss: 2.4474 - val_accuracy: 0.2143\n",
            "Epoch 18/100\n",
            "6/6 [==============================] - 9s 1s/step - loss: 1.6649 - accuracy: 0.2976 - val_loss: 2.4493 - val_accuracy: 0.2143\n",
            "Epoch 19/100\n",
            "6/6 [==============================] - 10s 2s/step - loss: 1.6657 - accuracy: 0.2679 - val_loss: 2.4377 - val_accuracy: 0.2143\n",
            "Epoch 20/100\n",
            "6/6 [==============================] - 10s 2s/step - loss: 1.6805 - accuracy: 0.2381 - val_loss: 2.4468 - val_accuracy: 0.2143\n",
            "Epoch 21/100\n",
            "6/6 [==============================] - 9s 2s/step - loss: 1.6802 - accuracy: 0.2976 - val_loss: 2.4342 - val_accuracy: 0.2143\n",
            "Epoch 22/100\n",
            "6/6 [==============================] - 10s 2s/step - loss: 1.6847 - accuracy: 0.2679 - val_loss: 2.4217 - val_accuracy: 0.2143\n",
            "Epoch 23/100\n",
            "6/6 [==============================] - 10s 2s/step - loss: 1.6935 - accuracy: 0.2619 - val_loss: 2.4275 - val_accuracy: 0.2143\n",
            "Epoch 24/100\n",
            "6/6 [==============================] - 10s 2s/step - loss: 1.6733 - accuracy: 0.2381 - val_loss: 2.4311 - val_accuracy: 0.1905\n",
            "Epoch 25/100\n",
            "6/6 [==============================] - 10s 2s/step - loss: 1.6904 - accuracy: 0.2381 - val_loss: 2.4174 - val_accuracy: 0.2143\n",
            "Epoch 26/100\n",
            "6/6 [==============================] - 9s 1s/step - loss: 1.6853 - accuracy: 0.2262 - val_loss: 2.4303 - val_accuracy: 0.2143\n",
            "Epoch 27/100\n",
            "6/6 [==============================] - 10s 2s/step - loss: 1.6960 - accuracy: 0.2917 - val_loss: 2.4353 - val_accuracy: 0.2143\n",
            "Epoch 28/100\n",
            "6/6 [==============================] - 10s 2s/step - loss: 1.6918 - accuracy: 0.2738 - val_loss: 2.4224 - val_accuracy: 0.2143\n",
            "Epoch 29/100\n",
            "6/6 [==============================] - 9s 1s/step - loss: 1.6887 - accuracy: 0.2619 - val_loss: 2.3696 - val_accuracy: 0.1905\n",
            "Epoch 30/100\n",
            "6/6 [==============================] - 10s 1s/step - loss: 1.6707 - accuracy: 0.2262 - val_loss: 2.3819 - val_accuracy: 0.2143\n",
            "Epoch 31/100\n",
            "6/6 [==============================] - 10s 2s/step - loss: 1.6782 - accuracy: 0.2321 - val_loss: 2.4014 - val_accuracy: 0.2143\n",
            "Epoch 32/100\n",
            "6/6 [==============================] - 10s 2s/step - loss: 1.6936 - accuracy: 0.1845 - val_loss: 2.4496 - val_accuracy: 0.1905\n",
            "Epoch 33/100\n",
            "6/6 [==============================] - 8s 1s/step - loss: 1.6806 - accuracy: 0.2500 - val_loss: 2.4513 - val_accuracy: 0.2143\n",
            "Epoch 34/100\n",
            "6/6 [==============================] - 10s 2s/step - loss: 1.6975 - accuracy: 0.2262 - val_loss: 2.4784 - val_accuracy: 0.1905\n",
            "Epoch 35/100\n",
            "6/6 [==============================] - 10s 2s/step - loss: 1.7154 - accuracy: 0.2619 - val_loss: 2.4499 - val_accuracy: 0.1905\n",
            "Epoch 36/100\n",
            "6/6 [==============================] - 11s 2s/step - loss: 1.6938 - accuracy: 0.2262 - val_loss: 2.4518 - val_accuracy: 0.2143\n",
            "Epoch 37/100\n",
            "6/6 [==============================] - 8s 1s/step - loss: 1.6922 - accuracy: 0.2679 - val_loss: 2.4395 - val_accuracy: 0.2143\n",
            "Epoch 38/100\n",
            "6/6 [==============================] - 10s 2s/step - loss: 1.6860 - accuracy: 0.2679 - val_loss: 2.4555 - val_accuracy: 0.2143\n",
            "Epoch 39/100\n",
            "6/6 [==============================] - 10s 2s/step - loss: 1.6914 - accuracy: 0.2440 - val_loss: 2.4519 - val_accuracy: 0.1905\n",
            "Epoch 40/100\n",
            "6/6 [==============================] - 9s 2s/step - loss: 1.6799 - accuracy: 0.2738 - val_loss: 2.4036 - val_accuracy: 0.2143\n",
            "Epoch 41/100\n",
            "6/6 [==============================] - 9s 1s/step - loss: 1.6878 - accuracy: 0.2738 - val_loss: 2.4418 - val_accuracy: 0.2143\n",
            "Epoch 42/100\n",
            "6/6 [==============================] - 10s 2s/step - loss: 1.6975 - accuracy: 0.2679 - val_loss: 2.3685 - val_accuracy: 0.2143\n",
            "Epoch 43/100\n",
            "6/6 [==============================] - 10s 2s/step - loss: 1.6843 - accuracy: 0.2560 - val_loss: 2.3713 - val_accuracy: 0.1905\n",
            "Epoch 44/100\n",
            "6/6 [==============================] - 8s 1s/step - loss: 1.6785 - accuracy: 0.2798 - val_loss: 2.4201 - val_accuracy: 0.1905\n",
            "Epoch 45/100\n",
            "6/6 [==============================] - 10s 2s/step - loss: 1.6709 - accuracy: 0.2738 - val_loss: 2.4329 - val_accuracy: 0.1905\n",
            "Epoch 46/100\n",
            "6/6 [==============================] - 10s 2s/step - loss: 1.6790 - accuracy: 0.2679 - val_loss: 2.4350 - val_accuracy: 0.2143\n",
            "Epoch 47/100\n",
            "6/6 [==============================] - 9s 2s/step - loss: 1.6886 - accuracy: 0.2679 - val_loss: 2.4430 - val_accuracy: 0.2143\n",
            "Epoch 48/100\n",
            "6/6 [==============================] - 9s 1s/step - loss: 1.7032 - accuracy: 0.2262 - val_loss: 2.4539 - val_accuracy: 0.1905\n",
            "Epoch 49/100\n",
            "6/6 [==============================] - 10s 2s/step - loss: 1.6838 - accuracy: 0.2560 - val_loss: 2.4522 - val_accuracy: 0.2143\n",
            "Epoch 50/100\n",
            "6/6 [==============================] - 11s 2s/step - loss: 1.6881 - accuracy: 0.2619 - val_loss: 2.4658 - val_accuracy: 0.2143\n",
            "Epoch 51/100\n",
            "6/6 [==============================] - 8s 1s/step - loss: 1.6795 - accuracy: 0.2679 - val_loss: 2.4731 - val_accuracy: 0.2143\n",
            "Epoch 52/100\n",
            "6/6 [==============================] - 9s 2s/step - loss: 1.6821 - accuracy: 0.2619 - val_loss: 2.4536 - val_accuracy: 0.2143\n",
            "Epoch 53/100\n",
            "6/6 [==============================] - 10s 2s/step - loss: 1.6789 - accuracy: 0.2679 - val_loss: 2.4409 - val_accuracy: 0.1905\n",
            "Epoch 54/100\n",
            "6/6 [==============================] - 9s 2s/step - loss: 1.6653 - accuracy: 0.2500 - val_loss: 2.4269 - val_accuracy: 0.1905\n",
            "Epoch 55/100\n",
            "6/6 [==============================] - 9s 1s/step - loss: 1.6859 - accuracy: 0.2619 - val_loss: 2.4333 - val_accuracy: 0.1905\n",
            "Epoch 56/100\n",
            "6/6 [==============================] - 10s 2s/step - loss: 1.6657 - accuracy: 0.2679 - val_loss: 2.4371 - val_accuracy: 0.2143\n",
            "Epoch 57/100\n",
            "6/6 [==============================] - 10s 2s/step - loss: 1.7071 - accuracy: 0.2738 - val_loss: 2.4522 - val_accuracy: 0.2143\n",
            "Epoch 58/100\n",
            "6/6 [==============================] - 8s 1s/step - loss: 1.6861 - accuracy: 0.2381 - val_loss: 2.4382 - val_accuracy: 0.2143\n",
            "Epoch 59/100\n",
            "6/6 [==============================] - 10s 2s/step - loss: 1.6945 - accuracy: 0.2500 - val_loss: 2.4894 - val_accuracy: 0.1905\n",
            "Epoch 60/100\n",
            "6/6 [==============================] - 10s 2s/step - loss: 1.6847 - accuracy: 0.2321 - val_loss: 2.4442 - val_accuracy: 0.1905\n",
            "Epoch 61/100\n",
            "6/6 [==============================] - 9s 2s/step - loss: 1.6898 - accuracy: 0.2619 - val_loss: 2.4271 - val_accuracy: 0.2143\n",
            "Epoch 62/100\n",
            "6/6 [==============================] - 9s 1s/step - loss: 1.6862 - accuracy: 0.2560 - val_loss: 2.3503 - val_accuracy: 0.1905\n",
            "Epoch 63/100\n",
            "6/6 [==============================] - 10s 2s/step - loss: 1.6866 - accuracy: 0.2381 - val_loss: 2.3041 - val_accuracy: 0.2143\n",
            "Epoch 64/100\n",
            "6/6 [==============================] - 11s 2s/step - loss: 1.7061 - accuracy: 0.1786 - val_loss: 2.3705 - val_accuracy: 0.2143\n",
            "Epoch 65/100\n",
            "6/6 [==============================] - 9s 1s/step - loss: 1.6834 - accuracy: 0.2560 - val_loss: 2.4387 - val_accuracy: 0.2143\n",
            "Epoch 66/100\n",
            "6/6 [==============================] - 10s 2s/step - loss: 1.6848 - accuracy: 0.2560 - val_loss: 2.4183 - val_accuracy: 0.2143\n",
            "Epoch 67/100\n",
            "6/6 [==============================] - 10s 2s/step - loss: 1.6864 - accuracy: 0.2024 - val_loss: 2.4155 - val_accuracy: 0.2143\n",
            "Epoch 68/100\n",
            "6/6 [==============================] - 9s 2s/step - loss: 1.6879 - accuracy: 0.2440 - val_loss: 2.4686 - val_accuracy: 0.1190\n",
            "Epoch 69/100\n",
            "6/6 [==============================] - 9s 1s/step - loss: 1.6990 - accuracy: 0.2381 - val_loss: 2.4474 - val_accuracy: 0.1905\n",
            "Epoch 70/100\n",
            "6/6 [==============================] - 10s 2s/step - loss: 1.6763 - accuracy: 0.2857 - val_loss: 2.4625 - val_accuracy: 0.2143\n",
            "Epoch 71/100\n",
            "6/6 [==============================] - 10s 2s/step - loss: 1.6755 - accuracy: 0.2440 - val_loss: 2.4924 - val_accuracy: 0.2143\n",
            "Epoch 72/100\n",
            "6/6 [==============================] - 9s 1s/step - loss: 1.6849 - accuracy: 0.2679 - val_loss: 2.4641 - val_accuracy: 0.2143\n",
            "Epoch 73/100\n",
            "6/6 [==============================] - 10s 2s/step - loss: 1.6932 - accuracy: 0.2619 - val_loss: 2.4194 - val_accuracy: 0.2143\n",
            "Epoch 74/100\n",
            "6/6 [==============================] - 10s 2s/step - loss: 1.6894 - accuracy: 0.2560 - val_loss: 2.4464 - val_accuracy: 0.1905\n",
            "Epoch 75/100\n",
            "6/6 [==============================] - 10s 2s/step - loss: 1.6845 - accuracy: 0.2381 - val_loss: 2.4504 - val_accuracy: 0.2143\n",
            "Epoch 76/100\n",
            "6/6 [==============================] - 9s 1s/step - loss: 1.6627 - accuracy: 0.2857 - val_loss: 2.3899 - val_accuracy: 0.2143\n",
            "Epoch 77/100\n",
            "6/6 [==============================] - 10s 2s/step - loss: 1.6670 - accuracy: 0.2679 - val_loss: 2.3362 - val_accuracy: 0.2143\n",
            "Epoch 78/100\n",
            "6/6 [==============================] - 11s 2s/step - loss: 1.6767 - accuracy: 0.2619 - val_loss: 2.3535 - val_accuracy: 0.2143\n",
            "Epoch 79/100\n",
            "6/6 [==============================] - 8s 1s/step - loss: 1.6786 - accuracy: 0.2560 - val_loss: 2.3576 - val_accuracy: 0.1905\n",
            "Epoch 80/100\n",
            "6/6 [==============================] - 10s 2s/step - loss: 1.6931 - accuracy: 0.2083 - val_loss: 2.3810 - val_accuracy: 0.2143\n",
            "Epoch 81/100\n",
            "6/6 [==============================] - 10s 2s/step - loss: 1.6826 - accuracy: 0.2619 - val_loss: 2.4292 - val_accuracy: 0.2143\n",
            "Epoch 82/100\n",
            "6/6 [==============================] - 9s 2s/step - loss: 1.6882 - accuracy: 0.2738 - val_loss: 2.3988 - val_accuracy: 0.2143\n",
            "Epoch 83/100\n",
            "6/6 [==============================] - 9s 1s/step - loss: 1.6691 - accuracy: 0.2440 - val_loss: 2.4265 - val_accuracy: 0.1905\n",
            "Epoch 84/100\n",
            "6/6 [==============================] - 10s 2s/step - loss: 1.6968 - accuracy: 0.2440 - val_loss: 2.4277 - val_accuracy: 0.2143\n",
            "Epoch 85/100\n",
            "6/6 [==============================] - 10s 2s/step - loss: 1.6742 - accuracy: 0.2440 - val_loss: 2.3868 - val_accuracy: 0.2143\n",
            "Epoch 86/100\n",
            "6/6 [==============================] - 9s 1s/step - loss: 1.6756 - accuracy: 0.2619 - val_loss: 2.3962 - val_accuracy: 0.2143\n",
            "Epoch 87/100\n",
            "6/6 [==============================] - 10s 1s/step - loss: 1.6817 - accuracy: 0.2560 - val_loss: 2.4023 - val_accuracy: 0.2143\n",
            "Epoch 88/100\n",
            "6/6 [==============================] - 10s 2s/step - loss: 1.6784 - accuracy: 0.2560 - val_loss: 2.4387 - val_accuracy: 0.1905\n",
            "Epoch 89/100\n",
            "6/6 [==============================] - 10s 2s/step - loss: 1.6856 - accuracy: 0.2381 - val_loss: 2.4522 - val_accuracy: 0.1905\n",
            "Epoch 90/100\n",
            "6/6 [==============================] - 9s 1s/step - loss: 1.6948 - accuracy: 0.2381 - val_loss: 2.4634 - val_accuracy: 0.2143\n",
            "Epoch 91/100\n",
            "6/6 [==============================] - 10s 2s/step - loss: 1.6791 - accuracy: 0.2500 - val_loss: 2.4742 - val_accuracy: 0.2143\n",
            "Epoch 92/100\n",
            "6/6 [==============================] - 11s 2s/step - loss: 1.6865 - accuracy: 0.2560 - val_loss: 2.4190 - val_accuracy: 0.1905\n",
            "Epoch 93/100\n",
            "6/6 [==============================] - 9s 2s/step - loss: 1.6970 - accuracy: 0.2381 - val_loss: 2.4147 - val_accuracy: 0.1905\n",
            "Epoch 94/100\n",
            "6/6 [==============================] - 9s 1s/step - loss: 1.6730 - accuracy: 0.2679 - val_loss: 2.3922 - val_accuracy: 0.1905\n",
            "Epoch 95/100\n",
            "6/6 [==============================] - 10s 2s/step - loss: 1.6681 - accuracy: 0.2679 - val_loss: 2.4254 - val_accuracy: 0.1905\n",
            "Epoch 96/100\n",
            "6/6 [==============================] - 10s 2s/step - loss: 1.6853 - accuracy: 0.2202 - val_loss: 2.4406 - val_accuracy: 0.1905\n",
            "Epoch 97/100\n",
            "6/6 [==============================] - 9s 1s/step - loss: 1.6834 - accuracy: 0.2440 - val_loss: 2.4812 - val_accuracy: 0.2143\n",
            "Epoch 98/100\n",
            "6/6 [==============================] - 10s 2s/step - loss: 1.6940 - accuracy: 0.2202 - val_loss: 2.4653 - val_accuracy: 0.2143\n",
            "Epoch 99/100\n",
            "6/6 [==============================] - 10s 2s/step - loss: 1.6744 - accuracy: 0.2738 - val_loss: 2.4595 - val_accuracy: 0.2143\n",
            "Epoch 100/100\n",
            "6/6 [==============================] - 10s 2s/step - loss: 1.6855 - accuracy: 0.1845 - val_loss: 2.4495 - val_accuracy: 0.1905\n",
            "Final Loss: 1.8321\n",
            "Final Accuracy: 0.2333\n",
            "LSTM model trained and saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow-addons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3ngFMT2Ia4z",
        "outputId": "81349ad6-bea9-4145-f399-c634d7839dd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.10/dist-packages (0.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (24.0)\n",
            "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (2.13.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RU1fs75pKc5J",
        "outputId": "678c8fcc-95ba-41dc-ece3-d61e1f09bbd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch torchvision torchaudio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMmGTqnMKmBt",
        "outputId": "9f1673e3-8aac-431c-9433-de226d50f258"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.17.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 3**"
      ],
      "metadata": {
        "id": "q9g1gxukR2EA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Define a custom dataset class\n",
        "class TaylorExpansionDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        function, expansions = self.data[idx]\n",
        "        input_text = f'Function: {function}\\n'\n",
        "        for order, expansion in enumerate(expansions, start=1):\n",
        "            input_text += f'Order {order} Taylor Expansion: {expansion}\\n'\n",
        "        encoding = self.tokenizer(input_text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(0),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(0)\n",
        "        }\n",
        "\n",
        "# Read taylor_dataset.txt\n",
        "with open('taylor_dataset.txt', 'r') as file:\n",
        "    data = file.readlines()\n",
        "\n",
        "# Parse taylor_dataset.txt to extract functions and their Taylor expansions\n",
        "functions = []\n",
        "taylor_expansions = []\n",
        "current_function = None\n",
        "current_expansions = []\n",
        "for line in data:\n",
        "    line = line.strip()\n",
        "    if line.startswith('Function:'):\n",
        "        if current_function is not None:\n",
        "            functions.append(current_function)\n",
        "            taylor_expansions.append(current_expansions)\n",
        "        current_function = line[len('Function:'):].strip()\n",
        "        current_expansions = []\n",
        "    elif line.startswith('Order'):\n",
        "        expansion = line.split(':', 1)[1].strip()\n",
        "        current_expansions.append(expansion)\n",
        "if current_function is not None:\n",
        "    functions.append(current_function)\n",
        "    taylor_expansions.append(current_expansions)\n",
        "\n",
        "# Split data into training and validation sets\n",
        "train_functions, val_functions, train_expansions, val_expansions = train_test_split(\n",
        "    functions, taylor_expansions, test_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "# Initialize tokenizer and model\n",
        "access_token = \"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\"\n",
        "\n",
        "# Initialize tokenizer and model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', token=access_token)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "config = GPT2Config.from_pretrained('gpt2', output_hidden_states=True)\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2', config=config)\n",
        "\n",
        "# Define maximum sequence length\n",
        "max_length = 256  # Reduced sequence length\n",
        "\n",
        "# Define training and validation datasets\n",
        "train_dataset = TaylorExpansionDataset(list(zip(train_functions, train_expansions)), tokenizer, max_length)\n",
        "val_dataset = TaylorExpansionDataset(list(zip(val_functions, val_expansions)), tokenizer, max_length)\n",
        "\n",
        "# Define dataloaders with reduced batch size\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)  # Reduced batch size\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False)  # Reduced batch size\n",
        "\n",
        "# Training loop\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Use GPU if available\n",
        "model.to(device)\n",
        "\n",
        "num_epochs = 3\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for batch in train_dataloader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_losses = []\n",
        "    for batch in val_dataloader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)\n",
        "            val_losses.append(outputs.loss.item())\n",
        "\n",
        "    val_loss = np.mean(val_losses)\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, Validation Loss: {val_loss}')\n",
        "\n",
        "# Save trained model\n",
        "model.save_pretrained('taylor_expansion_model')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InJKKGTGEaKM",
        "outputId": "2d3333b4-c7a1-4aa4-feff-2fecf6692979"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3, Validation Loss: 0.7030889987945557\n",
            "Epoch 2/3, Validation Loss: 0.5177750587463379\n",
            "Epoch 3/3, Validation Loss: 0.41907480359077454\n"
          ]
        }
      ]
    }
  ]
}